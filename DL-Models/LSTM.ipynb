{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Crop Price Prediction using LSTM\n",
    "\n",
    "This notebook demonstrates how to build and train an LSTM (Long Short-Term Memory) model to predict the modal price of bananas based on historical data. The process includes data loading, preprocessing, model training, evaluation, and making predictions for a specific date."
   ],
   "id": "a03150bde5dbcab7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Import Necessary Libraries\n",
   "id": "7ac450558c8d7cc0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta"
   ],
   "id": "99ff1a0601b3917a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Load and Preprocess the Data",
   "id": "80a6a9dadb2b9af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load the dataset\n",
    "# IMPORTANT: Make sure the 'Banana.csv' file is in the same directory as this notebook\n",
    "# or provide the correct path to the file.\n",
    "try:\n",
    "    df = pd.read_csv('../Weather_Merged_CSVs/Banana.csv')\n",
    "    print(\"Data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'Banana.csv' not found. Please ensure the file is in the correct directory.\")\n",
    "    # As a fallback for demonstration, creating a dummy dataframe.\n",
    "    # Replace this with your actual data loading.\n",
    "    data = {'District Name': ['Dharmapuri'], 'Market Name': ['AJattihalli(Farmers Market)'],\n",
    "            'Commodity': ['Banana'], 'Variety': ['Besrai'], 'Grade': ['Local'],\n",
    "            'Min Price (Rs./Quintal)': [4000.0], 'Max Price (Rs./Quintal)': [4500.0],\n",
    "            'Modal Price (Rs./Quintal)': [4500.0], 'Price Date': ['2025-06-18'],\n",
    "            'Day Of Week': [2], 'lookback_temp_mean': [27.77], 'lookback_precip_sum': [258.2]}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Convert 'Price Date' to datetime objects and set as index\n",
    "df['Price Date'] = pd.to_datetime(df['Price Date'])\n",
    "df.sort_values('Price Date', inplace=True)\n",
    "df.set_index('Price Date', inplace=True)\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"Original Data:\")\n",
    "print(df.head())"
   ],
   "id": "bcd6ff5683d5efca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Encode categorical features\n",
    "categorical_cols = ['District Name', 'Market Name', 'Commodity', 'Variety', 'Grade']\n",
    "encoders = {col: LabelEncoder() for col in categorical_cols}\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df[col] = encoders[col].fit_transform(df[col])\n",
    "\n",
    "print(\"\\nData after Label Encoding:\")\n",
    "print(df.head())"
   ],
   "id": "58ccebda4c9c81fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Feature Scaling\n",
    "# We will scale the numerical features to be between 0 and 1\n",
    "scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler() # Separate scaler for the target variable\n",
    "\n",
    "# We'll use all columns as features for prediction\n",
    "features = df.columns\n",
    "df_scaled = scaler.fit_transform(df[features])\n",
    "\n",
    "# Fit the target scaler on the 'Modal Price' column for inverse transforming later\n",
    "modal_price_index = df.columns.get_loc('Modal Price (Rs./Quintal)')\n",
    "target_scaler.fit(df[['Modal Price (Rs./Quintal)']])\n",
    "\n",
    "# Create a new DataFrame with scaled values\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=features, index=df.index)\n",
    "\n",
    "print(\"\\nData after Scaling:\")\n",
    "print(df_scaled.head())"
   ],
   "id": "775036a8024396e6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Create Time-Series Sequences\n",
    "\n",
    "LSTM models require input data in the form of sequences. We will create sequences of a fixed length (lookback period) to predict the next time step's price."
   ],
   "id": "da6e5c1e5e8c451b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_sequences(data, lookback, target_col_index):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - lookback):\n",
    "        X.append(data[i:(i + lookback)])\n",
    "        y.append(data[i + lookback, target_col_index])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define the lookback period\n",
    "lookback = 120 # Use 120 days of data to predict the next day's price\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(df_scaled.values, lookback, modal_price_index)\n",
    "\n",
    "print(f\"\\nShape of X: {X.shape}\")\n",
    "print(f\"Shape of y: {y.shape}\")"
   ],
   "id": "93d25119d8852511"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Split Data and Create DataLoaders",
   "id": "28af70796b16d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train, y_test = y[:split_index], y[split_index:]\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "batch_size = 64\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "id": "8aa4bd7d3079ae81"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Define the LSTM Model",
   "id": "e56320a0d05402dd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.3):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # LSTM layer with dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "        # Fully connected layer to get the final output\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "\n",
    "        # We need to detach as we are doing truncated backpropagation through time (BPTT)\n",
    "        out, _ = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # We only want the output from the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Model parameters\n",
    "input_size = X_train.shape[2] # Number of features\n",
    "hidden_size = 500  # Increased hidden size\n",
    "num_layers = 3     # Increased number of layers\n",
    "output_size = 1\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout=0.3).to(device)\n",
    "print(model)"
   ],
   "id": "e69cc42b295a74d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Train the Model",
   "id": "96a4823a568281f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # Lower learning rate\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30  # Increased epochs\n",
    "batch_size = 32  # Reduced batch size for more updates\n",
    "train_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, (sequences, labels) in enumerate(train_loader):\n",
    "        # Move tensors to the configured device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_loader)\n",
    "    train_losses.append(avg_epoch_loss)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_epoch_loss:.6f}')"
   ],
   "id": "f4132a9649d042b8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Evaluate the Model",
   "id": "f01418c106172671"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model.eval()\n",
    "all_predictions = []\n",
    "all_actuals = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in test_loader:\n",
    "        # Move tensors to the configured device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(sequences)\n",
    "        all_predictions.append(outputs.cpu().numpy()) # Move predictions to CPU for numpy conversion\n",
    "        all_actuals.append(labels.cpu().numpy()) # Move labels to CPU for numpy conversion\n",
    "\n",
    "# Concatenate all batches\n",
    "predictions_scaled = np.concatenate(all_predictions)\n",
    "actuals_scaled = np.concatenate(all_actuals)\n",
    "\n",
    "# Inverse transform the predictions and actuals to get original price values\n",
    "predictions = target_scaler.inverse_transform(predictions_scaled)\n",
    "actuals = target_scaler.inverse_transform(actuals_scaled)\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "rmse = np.sqrt(mean_squared_error(actuals, predictions))\n",
    "# Calculate percentage accuracy\n",
    "mean_actual = np.mean(actuals)\n",
    "accuracy = 100 * (1 - (rmse / mean_actual))\n",
    "print(f'\\nTest Root Mean Squared Error (RMSE): {rmse:.2f} Rs./Quintal')\n",
    "print(f'Model Percentage Accuracy: {accuracy:.2f}%')"
   ],
   "id": "9b0e9916cb8956e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "c1d03aa28efc1dd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Get the dates corresponding to the test set\n",
    "test_dates = df.index[split_index + lookback:]\n",
    "\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.scatter(test_dates, actuals, label='Actual Prices', color='blue', marker='o', alpha=0.7)\n",
    "plt.plot(test_dates, predictions, label='Predicted Prices', color='red', linestyle='--', linewidth=2)\n",
    "plt.fill_between(test_dates, np.squeeze(predictions), np.squeeze(actuals), color='gray', alpha=0.2, label='Difference')\n",
    "plt.title('Banana Price Prediction: Actual vs. Predicted (Scatter + Line)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Modal Price (Rs./Quintal)')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=':')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1fb5a034c0c3da58"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 9. Predict Price for a Specific Date",
   "id": "ff07fcee732d2613"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def predict_price(target_date_str):\n",
    "    \"\"\"\n",
    "    Predicts the modal price for a given date.\n",
    "    The function requires the `lookback` period of historical data before the target date.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        target_date = datetime.strptime(target_date_str, '%Y-%m-%d')\n",
    "    except ValueError:\n",
    "        return \"Invalid date format. Please use YYYY-MM-DD.\"\n",
    "\n",
    "    # Find the end date for the required historical data\n",
    "    end_date = target_date - timedelta(days=1)\n",
    "    start_date = end_date - timedelta(days=lookback - 1)\n",
    "\n",
    "    # Get the last 'lookback' days of data from our original dataframe\n",
    "    # We use the original df to get the data before scaling\n",
    "    last_sequence_df = df.loc[start_date:end_date]\n",
    "\n",
    "    if len(last_sequence_df) < lookback:\n",
    "        return f\"Not enough historical data to predict for {target_date_str}. Required {lookback} days, found {len(last_sequence_df)}.\"\n",
    "\n",
    "    # Ensure we have exactly 'lookback' days\n",
    "    last_sequence_df = last_sequence_df.tail(lookback)\n",
    "\n",
    "    # Scale the sequence\n",
    "    last_sequence_scaled = scaler.transform(last_sequence_df)\n",
    "\n",
    "    # Convert to tensor, add batch dimension, and move to the device\n",
    "    sequence_tensor = torch.tensor(last_sequence_scaled, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "    # Make prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction_scaled = model(sequence_tensor)\n",
    "\n",
    "    # Inverse transform the prediction (move to CPU for numpy conversion)\n",
    "    predicted_price = target_scaler.inverse_transform(prediction_scaled.cpu().numpy())\n",
    "\n",
    "    return f\"Predicted Modal Price for {target_date_str}: {predicted_price[0][0]:.2f} Rs./Quintal\"\n",
    "\n",
    "# Example: Predict price for a date\n",
    "# Note: The date must be after the available historical data allows.\n",
    "# Let's try to predict the day after our last known data point.\n",
    "last_date_in_data = df.index.max()\n",
    "prediction_date = (last_date_in_data + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "print(predict_price('2025-07-01'))\n",
    "print(predict_price('2025-09-15'))"
   ],
   "id": "f3773a31dad0ff9e"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
